# Summer-Internship2024-22AIML039
May 13th 2024
## Introduction
I have done my intenship at Mamo Technolabs as a Python Developer. My internship focuses on enhancing my Python programming skills through hands-on projects and real-world applications. At Mamo Technolabs, I collaborate on innovative software solutions, gaining experience in web development, data analysis, and automation. This opportunity allows me to learn industry best practices and contribute to impactful projects.

### Responsibility
My responsibilities include developing and maintaining Python-based applications, collaborating with the development team on various projects, writing and optimizing code for performance, and debugging and troubleshooting software issues. This internship allows me to gain practical experience and enhance my skills in a professional setting.

# Weekly Progress
## Week 1: May 13th - May 18th
My first task involves extracting company profiles and LinkedIn profiles from the website clutch.co. This entails scraping relevant data, ensuring data accuracy, and addressing any errors that arise during the extraction process. Utilizing Python libraries such as BeautifulSoup and Scrapy.
This project will help me hone my web scraping and data handling skills, ensuring that I deliver accurate and reliable results while adhering to best practices in ethical web scraping and data privacy.
### Define the Scope:
Identify the specific data points needed from Clutch.co, such as company names, profiles, LinkedIn URLs, reviews, ratings, services offered, and contact information.
### Set Up the Environment:
Prepare the development environment with the necessary Python libraries and tools, including BeautifulSoup, Scrapy, Requests, and Pandas for data manipulation.
## Week 2: May 20th - May 25th
### Web Scraping Implementation:
Crawling: Use Scrapy to navigate through the websiteâ€™s structure, ensuring efficient and comprehensive data collection.
Parsing: Employ BeautifulSoup to parse the HTML content and extract relevant information.
### Data Storage and Management:
Store the scraped data in a structured format, such as CSV or a database (e.g., SQLite, MongoDB), ensuring easy access and analysis. Regularly update and maintain the dataset to reflect the latest information from Clutch.co.
Cross-check the extracted data with source information to ensure accuracy.
Use validation techniques to clean and standardize the data, removing duplicates and inconsistencies.
Internship
![image](https://github.com/22vidhi/Summer-Internship2024-22AIML039/assets/120238387/8b65c5a1-56c3-47fe-a2fd-43481a8d9d34)

## Week 3: May 27th - Jaun 1st
A detailed Google search query was executed to find email addresses of individuals with roles such as CEO, CFO, and HR, specifically in the cyber security field and located in Boston. The search targeted email addresses from domains like gmail.com, hotmail.com, and yahoo.com.
Using an Email Extraction Tool:
An email extraction tool or browser extension was utilized to automatically extract email addresses from the search results, identifying a total of 46 email addresses.
The tool streamlined the data collection process by automatically scanning and extracting relevant email addresses, reducing the need for manual effort and increasing efficiency.
The extracted email addresses were displayed in a sidebar, with options to copy and export the data in various formats (TXT, CSV), facilitating easy integration into further data analysis or outreach activities.
![image](https://github.com/22vidhi/Summer-Internship2024-22AIML039/assets/120238387/827fe608-15cc-447d-8929-fd2c44687740)


## Week 4: June 3rd - June 8th
The code sets up a connection to a PostgreSQL database using credentials stored in environment variables. These variables are loaded using the dotenv library, ensuring secure handling of sensitive information. The connection string is then used to create an engine for database interactions.
Data Fetching and Updating Excel Sheet:
The code includes functions to fetch data from specified tables in the database and update an Excel sheet with this data using the xlwings library. The fetch_data_from_database function retrieves all rows nd column names from the selected table, and update_excel_sheet writes this data into the specified Excel sheet.
## Week 5: June 10th - June 15th
Uploading Data from Excel to Database:
Another function, main_u, reads data from an Excel sheet, processes it into a pandas DataFrame, and uploads it to the specified table in the database. This includes converting column names to lowercase and handling potential exceptions with appropriate error messages using the win32api library.
I have start work on project of AI ChatBot for the company. First company give the individual task for the project.
## Week 6: June 17th - June 22th
Skill Learned
Proficiency in using Python libraries like BeautifulSoup and Scrapy for web scraping and extracting data from websites.
Experience in connecting to and managing databases, including tasks such as uploading and retrieving data using libraries like SQLAlchemy.
Skills in automating data upload to Excel and loading data from Excel to a database using tools like xlwings.
Ability to handle and process data efficiently, ensuring accuracy and usability, and automating repetitive tasks to improve workflow efficiency.
These skills will be valuable for various roles in data science, software development, and database management.
## Conclusion
Overall, this internship has been a valuable learning experience. I gained practical skills in python development, visualization, and automation using Python. The projects I worked on have provided me with insights into real-world data challenges and solutions. I am grateful for the opportunity to contribute to the team and grow as a Python Developer.
